{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OCoDQ-w3GmBt"
      },
      "outputs": [],
      "source": [
        "#!pip install transformers datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vJvifJQ_G5dX"
      },
      "outputs": [],
      "source": [
        "#!pip install torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dv1UH9R-J72z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6ecb6a98-9284-4da6-fc90-9dcd1fe8cbf9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:88: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at dbmdz/bert-base-turkish-128k-cased and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ],
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "pipe = pipeline(\"question-answering\", model=\"dbmdz/bert-base-turkish-128k-cased\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KjSecRAGFcab",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cb5bbad0-5dc0-4e81-9216-542839abf76d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at dbmdz/bert-base-turkish-128k-cased and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BertForQuestionAnswering(\n",
            "  (bert): BertModel(\n",
            "    (embeddings): BertEmbeddings(\n",
            "      (word_embeddings): Embedding(128000, 768, padding_idx=0)\n",
            "      (position_embeddings): Embedding(512, 768)\n",
            "      (token_type_embeddings): Embedding(2, 768)\n",
            "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "      (dropout): Dropout(p=0.1, inplace=False)\n",
            "    )\n",
            "    (encoder): BertEncoder(\n",
            "      (layer): ModuleList(\n",
            "        (0-11): 12 x BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            (intermediate_act_fn): GELUActivation()\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (qa_outputs): Linear(in_features=768, out_features=2, bias=True)\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForQuestionAnswering\n",
        "from transformers import DefaultDataCollator\n",
        "from torchsummary import summary\n",
        "\n",
        "#tokenizer = AutoTokenizer.from_pretrained(\"husnu/bert-base-turkish-128k-cased-finetuned_lr-2e-05_epochs-3\")\n",
        "#model = AutoModelForQuestionAnswering.from_pretrained(\"husnu/bert-base-turkish-128k-cased-finetuned_lr-2e-05_epochs-3\")\n",
        "#tokenizer = AutoTokenizer.from_pretrained(\"dbmdz/bert-base-turkish-128k-cased\")\n",
        "#model = AutoModelForQuestionAnswering.from_pretrained(\"dbmdz/bert-base-turkish-128k-cased\")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"dbmdz/bert-base-turkish-128k-cased\")\n",
        "model = AutoModelForQuestionAnswering.from_pretrained(\"dbmdz/bert-base-turkish-128k-cased\")\n",
        "\n",
        "print(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iIqBn2tqlual",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "071f836c-4669-452c-dadc-b3480c9997cc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "import json\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "with open(r\"/content/drive/MyDrive/Wiki_Dataset_Final.json\", \"r\") as read_file:\n",
        "  data_dict = json.load(read_file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UnCjpMTm00co",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dc9aa823-6b45-45da-a789-cf14d8f9b964"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Token indices sequence length is longer than the specified maximum sequence length for this model (566 > 512). Running this sequence through the model will result in indexing errors\n"
          ]
        }
      ],
      "source": [
        "contexts = []\n",
        "questions = []\n",
        "answers = []\n",
        "\n",
        "for entry in data_dict['data']:\n",
        "  text = entry['text']\n",
        "  input_ids = tokenizer.encode(text, add_special_tokens=True)\n",
        "\n",
        "  if (len(input_ids) <= 512):\n",
        "\n",
        "    for qa in entry['qas']:\n",
        "      question = qa['question']\n",
        "      answer = qa['answer']\n",
        "      answer_start = qa['answer']\n",
        "      answer_start = qa['answer_start']\n",
        "      answer_end = qa['answer_end']\n",
        "\n",
        "      answer_start_end_dict = { }\n",
        "      answer_start_end_dict['answer'] = answer\n",
        "      answer_start_end_dict['answer_start'] = answer_start\n",
        "      answer_start_end_dict['answer_end'] = answer_end\n",
        "\n",
        "      contexts.append(text)\n",
        "      questions.append(question)\n",
        "      answers.append(answer_start_end_dict)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "max_len = 0\n",
        "\n",
        "for text in contexts:\n",
        "\n",
        "    input_ids = tokenizer.encode(text, add_special_tokens=True)\n",
        "\n",
        "    max_len = max(max_len, len(input_ids))\n",
        "\n",
        "print('Max sentence length: ', max_len)\n"
      ],
      "metadata": {
        "id": "R1tOkRbozPff",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2256e320-b3a2-40db-c4ef-e94064918f49"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Max sentence length:  502\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gCZTVUGWVDl0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1a236602-d069-49e5-d0c2-2fe7e938f275"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4707\n",
            "248\n"
          ]
        }
      ],
      "source": [
        "\n",
        "train_contexts = contexts[: int(len(contexts) * 0.95)]\n",
        "val_contexts = contexts[int(len(contexts) * 0.95) :]\n",
        "train_answers = answers[: int(len(answers) * 0.95)]\n",
        "val_answers = answers[int(len(answers) * 0.95) :]\n",
        "train_questions = questions[: int(len(questions) * 0.95)]\n",
        "val_questions = questions[int(len(contexts) * 0.95) :]\n",
        "\"\"\"\n",
        "train_contexts = contexts\n",
        "train_answers = answers\n",
        "train_questions = questions\n",
        "\"\"\"\n",
        "print(len(train_questions))\n",
        "print(len(val_questions))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_Tp-iezU5l3m",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8714d1a5-aab2-4739-f8c6-8b0488b9b3db"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "input_ids\n",
            "token_type_ids\n",
            "attention_mask\n"
          ]
        }
      ],
      "source": [
        "train_encodings = tokenizer(train_contexts, train_questions, truncation=True, padding=True)\n",
        "val_encodings = tokenizer(val_contexts, val_questions, truncation=True, padding=True)\n",
        "\n",
        "for key, value in train_encodings.items() :\n",
        "    print (key)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aK2ah-jii8iC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f6072786-7563-40a1-d886-9f93b95644d7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "input_ids\n",
            "token_type_ids\n",
            "attention_mask\n",
            "start_positions\n",
            "end_positions\n"
          ]
        }
      ],
      "source": [
        "\"\"\"\n",
        "print(\"val answers:\")\n",
        "for i in range(50):\n",
        "  print(train_answers[i])\n",
        "\n",
        "print(\"answers len:\")\n",
        "print(len(answers))\n",
        "#print(len(train_encodings.items()))\n",
        "\"\"\"\n",
        "\n",
        "def add_token_positions(encodings, answers):\n",
        "  answer_start_positions = []\n",
        "  answer_end_positions = []\n",
        "\n",
        "  for i in range(len(answers)):\n",
        "    answer_start_positions.append(encodings.char_to_token(i, answers[i]['answer_start']))\n",
        "    answer_end_positions.append(encodings.char_to_token(i, answers[i]['answer_end']))\n",
        "\n",
        "    if answer_start_positions[-1] is None:\n",
        "      answer_start_positions[-1] = tokenizer.model_max_length\n",
        "    if answer_end_positions[-1] is None:\n",
        "      answer_end_positions[-1] = tokenizer.model_max_length\n",
        "\n",
        "    encodings.update({'start_positions': answer_start_positions, 'end_positions': answer_end_positions})\n",
        "\n",
        "add_token_positions(train_encodings, train_answers)\n",
        "add_token_positions(val_encodings, val_answers)\n",
        "\n",
        "for key, value in train_encodings.items() :\n",
        "    print (key)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CBewREdWTHi9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3f9442ac-1d4b-4ea9-e25e-18a3080f0ead"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4707\n",
            "5\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "class SquadDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, encodings):\n",
        "        self.encodings = encodings\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "      item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
        "      return item\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.encodings.input_ids)\n",
        "\n",
        "\n",
        "train_dataset = SquadDataset(train_encodings)\n",
        "val_dataset = SquadDataset(val_encodings)\n",
        "#print(train_dataset[0])\n",
        "print(len(train_dataset))\n",
        "print(len(train_encodings))\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ItCans9sFc0H",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "outputId": "f17fd4af-3e91-4a86-b675-eda141cdbe05"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'from transformers import TrainingArguments, Trainer\\n\\ntrain_args = TrainingArguments(\\n    output_dir=\"bert-finetune-bitirme-16batch\",\\n    evaluation_strategy=\"epoch\",\\n    learning_rate=2e-5,\\n    per_device_train_batch_size=16,\\n    per_device_eval_batch_size=16,\\n    num_train_epochs=3,\\n    weight_decay=0.01,\\n)\\n\\ntrainer = Trainer(\\n    model=model,\\n    args=train_args,\\n    train_dataset=train_dataset,\\n    eval_dataset=val_dataset,\\n\\n)'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "zk9aAimEg3KC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "03xlFClkfUjM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "afa4d0f6-546b-4388-e143-946b253c792d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "\n",
            "Tesla T4\n",
            "Memory Usage:\n",
            "Allocated: 2.8 GB\n",
            "Cached:    12.8 GB\n",
            "295\n",
            "4707\n",
            "Epoch 0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 0: Loss = 1.5224076509475708\n",
            "Batch 10: Loss = 1.0995242595672607\n",
            "Batch 20: Loss = 0.850131094455719\n",
            "Batch 30: Loss = 1.2519258260726929\n",
            "Batch 40: Loss = 0.8630661368370056\n",
            "Batch 50: Loss = 1.0343854427337646\n",
            "Batch 60: Loss = 1.3816474676132202\n",
            "Batch 70: Loss = 1.0806645154953003\n",
            "Batch 80: Loss = 0.3219192326068878\n",
            "Batch 90: Loss = 1.9127004146575928\n",
            "Batch 100: Loss = 1.104391098022461\n",
            "Batch 110: Loss = 1.191523551940918\n",
            "Batch 120: Loss = 0.9998846054077148\n",
            "Batch 130: Loss = 1.3146454095840454\n",
            "Batch 140: Loss = 1.023534893989563\n",
            "Batch 150: Loss = 0.8943449258804321\n",
            "Batch 160: Loss = 1.6401782035827637\n",
            "Batch 170: Loss = 0.9291471242904663\n",
            "Batch 180: Loss = 0.9780365228652954\n",
            "Batch 190: Loss = 0.26940563321113586\n",
            "Batch 200: Loss = 1.210416316986084\n",
            "Batch 210: Loss = 1.1520577669143677\n",
            "Batch 220: Loss = 0.23170311748981476\n",
            "Batch 230: Loss = 0.20479337871074677\n",
            "Batch 240: Loss = 0.2758384346961975\n",
            "Batch 250: Loss = 0.4890936315059662\n",
            "Batch 260: Loss = 0.5077285766601562\n",
            "Batch 270: Loss = 1.9850739240646362\n",
            "Batch 280: Loss = 0.9548953771591187\n",
            "Batch 290: Loss = 1.010125756263733\n",
            "Epoch 1\n",
            "Batch 0: Loss = 1.497484803199768\n",
            "Batch 10: Loss = 0.6645731925964355\n",
            "Batch 20: Loss = 0.36519962549209595\n",
            "Batch 30: Loss = 2.045339345932007\n",
            "Batch 40: Loss = 0.9780077934265137\n",
            "Batch 50: Loss = 0.3187987208366394\n",
            "Batch 60: Loss = 0.2749529480934143\n",
            "Batch 70: Loss = 0.13957981765270233\n",
            "Batch 80: Loss = nan\n",
            "Batch 90: Loss = 1.483293890953064\n",
            "Batch 100: Loss = 1.3622181415557861\n",
            "Batch 110: Loss = 0.9331284165382385\n",
            "Batch 120: Loss = 0.218383327126503\n",
            "Batch 130: Loss = 0.9246419668197632\n",
            "Batch 140: Loss = 0.7116616368293762\n",
            "Batch 150: Loss = 0.46412938833236694\n",
            "Batch 160: Loss = 0.3292352855205536\n",
            "Batch 170: Loss = 0.15654654800891876\n",
            "Batch 180: Loss = 0.0616023875772953\n",
            "Batch 190: Loss = 0.7718287706375122\n",
            "Batch 200: Loss = 0.578115701675415\n",
            "Batch 210: Loss = 0.6560911536216736\n",
            "Batch 220: Loss = 0.6387338638305664\n",
            "Batch 230: Loss = 0.29298821091651917\n",
            "Batch 240: Loss = 0.5674314498901367\n",
            "Batch 250: Loss = 0.6037508249282837\n",
            "Batch 260: Loss = 0.624290943145752\n",
            "Batch 270: Loss = 0.869040846824646\n",
            "Batch 280: Loss = 0.9121347665786743\n",
            "Batch 290: Loss = 0.6074337959289551\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BertForQuestionAnswering(\n",
              "  (bert): BertModel(\n",
              "    (embeddings): BertEmbeddings(\n",
              "      (word_embeddings): Embedding(128000, 768, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (token_type_embeddings): Embedding(2, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): BertEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0-11): 12 x BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (qa_outputs): Linear(in_features=768, out_features=2, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "from transformers import AdamW\n",
        "import pdb\n",
        "import torch\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print('Using device:', device)\n",
        "print()\n",
        "\n",
        "#Additional Info when using cuda\n",
        "if device.type == 'cuda':\n",
        "    print(torch.cuda.get_device_name(0))\n",
        "    print('Memory Usage:')\n",
        "    print('Allocated:', round(torch.cuda.memory_allocated(0)/1024**3,1), 'GB')\n",
        "    print('Cached:   ', round(torch.cuda.memory_reserved(0)/1024**3,1), 'GB')\n",
        "\n",
        "model.to(device)\n",
        "model.train()\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
        "print(len(train_loader))\n",
        "print(len(train_dataset))\n",
        "\n",
        "optim = AdamW(model.parameters(), lr=2e-5)\n",
        "\n",
        "#print(len(train_loader))\n",
        "\n",
        "for epoch in range(2):\n",
        "    print(\"Epoch {}\".format(epoch))\n",
        "    batch_num = 0\n",
        "    for batch in train_loader:\n",
        "        optim.zero_grad()\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        token_type_ids = batch['token_type_ids'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "        start_positions = batch['start_positions'].to(device)\n",
        "        end_positions = batch['end_positions'].to(device)\n",
        "        #outputs = model.bert(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n",
        "        outputs = model(input_ids=input_ids, token_type_ids=token_type_ids, attention_mask=attention_mask, start_positions=start_positions, end_positions=end_positions)\n",
        "        #outputs = model.bert(input_ids, attention_mask=attention_mask, answer_start=start_positions, answer_end=end_positions)\n",
        "        loss = outputs[0]\n",
        "        #loss.mean().backward()\n",
        "        loss.backward()\n",
        "        optim.step()\n",
        "        if batch_num % 10 == 0:\n",
        "          print(\"Batch {}: Loss = {}\".format(batch_num, loss.item()))\n",
        "        batch_num += 1\n",
        "\n",
        "model.eval()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_model(model, dataloader, device):\n",
        "    model.eval()\n",
        "    correct_predictions = 0\n",
        "    total_samples = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in dataloader:\n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            token_type_ids = batch['token_type_ids'].to(device)\n",
        "            attention_mask = batch['attention_mask'].to(device)\n",
        "            start_positions = batch['start_positions'].to(device)\n",
        "            end_positions = batch['end_positions'].to(device)\n",
        "\n",
        "            outputs = model(input_ids=input_ids, token_type_ids=token_type_ids,\n",
        "                            attention_mask=attention_mask, start_positions=start_positions, end_positions=end_positions)\n",
        "            #loss, start_scores, end_scores = outputs\n",
        "            loss = outputs[0]\n",
        "            start_scores = outputs[1]\n",
        "            end_scores = outputs[2]\n",
        "\n",
        "            predicted_start = torch.argmax(start_scores, dim=1)\n",
        "            predicted_end = torch.argmax(end_scores, dim=1)\n",
        "\n",
        "            correct_predictions += ((predicted_start == start_positions) & (predicted_end == end_positions)).sum().item()\n",
        "            total_samples += start_positions.size(0)\n",
        "\n",
        "    accuracy = correct_predictions / total_samples\n",
        "    return accuracy\n",
        "\n",
        "model.to(device)\n",
        "val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False)\n",
        "\n",
        "accuracy = evaluate_model(model, val_loader, device)\n",
        "print(\"Test Accuracy: {:.2%}\".format(accuracy))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5DMOoJQ9zBXY",
        "outputId": "5a53a028-a2ce-422e-b94c-e9d47b5fa82c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy: 9.68%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import f1_score\n",
        "\n",
        "val_loader = DataLoader(train_dataset, batch_size=16)\n",
        "\n",
        "def evaluate_model_f1(model, dataloader, device):\n",
        "    model.eval()\n",
        "    all_true_start = []\n",
        "    all_pred_start = []\n",
        "    all_true_end = []\n",
        "    all_pred_end = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in dataloader:\n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            token_type_ids = batch['token_type_ids'].to(device)\n",
        "            attention_mask = batch['attention_mask'].to(device)\n",
        "            start_positions = batch['start_positions'].to(device)\n",
        "            end_positions = batch['end_positions'].to(device)\n",
        "\n",
        "            outputs = model(input_ids=input_ids, token_type_ids=token_type_ids, attention_mask=attention_mask, start_positions=start_positions, end_positions=end_positions)\n",
        "            loss = outputs[0]\n",
        "            start_scores = outputs[1]\n",
        "            end_scores = outputs[2]\n",
        "\n",
        "            predicted_start = torch.argmax(start_scores, dim=1)\n",
        "            predicted_end = torch.argmax(end_scores, dim=1)\n",
        "\n",
        "            all_true_start.extend(start_positions.cpu().numpy())\n",
        "            all_pred_start.extend(predicted_start.cpu().numpy())\n",
        "            all_true_end.extend(end_positions.cpu().numpy())\n",
        "            all_pred_end.extend(predicted_end.cpu().numpy())\n",
        "\n",
        "    f1_start = f1_score(all_true_start, all_pred_start, average='micro')\n",
        "    f1_end = f1_score(all_true_end, all_pred_end, average='micro')\n",
        "    f1_score_total = (f1_start + f1_end) / 2.0\n",
        "\n",
        "    return f1_score_total\n",
        "\n",
        "model.to(device)\n",
        "f1_score_result = evaluate_model_f1(model, val_loader, device)\n",
        "print(\"Test F1 Score: {:.2%}\".format(f1_score_result))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "01nW4y4400Vl",
        "outputId": "58f505d4-30d1-477c-d65d-0e59adbf2bf8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test F1 Score: 56.22%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"from huggingface_hub import notebook_login\n",
        "\n",
        "notebook_login()\n",
        "\n",
        "model.push_to_hub(\"ykakca/bert-bitirme-128k\")\n",
        "tokenizer.push_to_hub(\"ykakca/bert-bitirme-128k\")\"\"\""
      ],
      "metadata": {
        "id": "RkukXV3CivQW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7ApfFxuXkcQF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "MB_hAp4atLxG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "pQWZlnSF2uBn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\"\"\"\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "from transformers import AutoTokenizer, AutoModelForQuestionAnswering\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"ykakca/bert-ytu-bitirme-fine-tuned\")\n",
        "model = AutoModelForQuestionAnswering.from_pretrained(\"ykakca/bert-ytu-bitirme-fine-tuned\")\n",
        "model = model.to(device)\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "jvh814VuLe0T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "V5uQ2QWBCgsc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "CfQhTt5a-QOn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Y7IFMtn2Lsol"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ctV2cEC089PG"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}